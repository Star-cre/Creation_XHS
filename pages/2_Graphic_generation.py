from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch
import streamlit as st
from modelscope import snapshot_download

# ä¾§è¾¹æ ä¸­åˆ›å»ºæ ‡é¢˜å’Œé“¾æ¥
with st.sidebar:
    st.markdown("## InternLM LLM")
    "[InternLM](https://github.com/InternLM/InternLM.git)"
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    "[![å°çº¢ä¹¦ç¾å¦†æ–‡æ¡ˆç”Ÿæˆå¯¼å¸ˆ](https://github.com/codespaces/badge.svg)](https://github.com/Star-cre/Creation_XHS)"
    system_prompt = st.text_input(
        "System_Prompt", """
        ä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦æ–‡æ¡ˆç”Ÿæˆå™¨ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹è§„åˆ™ç”Ÿæˆå°çº¢ä¹¦æ–‡æ¡ˆï¼š\n
        - ä¸»é¢˜/äº§å“ï¼šxxï¼ˆåœ¨è¿™é‡Œå¡«å†™å…·ä½“çš„ç¾å¦†äº§å“åç§°æˆ–ç±»åˆ«ï¼‰\n
        - éœ€æ±‚ï¼šæ’°å†™ä¸€ç¯‡å…³äºxxçš„å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆï¼Œçªå‡ºå…¶ç‰¹ç‚¹å’Œä½¿ç”¨ä½“éªŒ\n
        - é£æ ¼ï¼šå£è¯­åŒ–ã€ç”ŸåŠ¨æ´»æ³¼ï¼Œä½¿ç”¨Emojiè¡¨æƒ…å›¾æ ‡ï¼Œå¸å¼•è¯»è€…æ³¨æ„\n
        - é™åˆ¶ï¼šæ–‡æ¡ˆé•¿åº¦æ§åˆ¶åœ¨500å­—ä»¥å†…ï¼Œé¿å…è¿ç»­æ€§æ ‡é¢˜ç»“æ„ï¼Œä¸»è¦ä»¥ä¸­æ–‡æ€ç»´æ–¹å¼æ’°å†™\n
        è¯·ä¸è¦è¾“å‡ºå¤šä½™çš„æ–‡å­—ï¼Œä¸»è¾“å‡ºæ–‡æ¡ˆæœ¬ä½“\n
        ä¸‹è¾¹çš„[]å†…ç»™å‡ºéœ€è¦ç”Ÿæˆçš„å°çº¢ä¹¦æ–‡æ¡ˆä¸»é¢˜/äº§å“\n
        """)

# è®¾ç½®æ ‡é¢˜å’Œå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Chatbot: å°çº¢ä¹¦å›¾æ–‡ç”Ÿæˆ")
st.caption("ğŸš€ A streamlit chatbot powered by InternLM LLM")

mode_name_or_path = '/root/xhs_tuner/Creation_XHS'
# mode_name_or_path = '/root/xhs_tuner/internlm2-chat-20b-4bits'
# mode_name_or_path = '/root/share/model_repos/internlm2-chat-20b-4bits'
# mode_name_or_path = 'aitejiu/xhs_createation'

@st.cache_resource
def get_model():
    tokenizer = AutoTokenizer.from_pretrained(
        mode_name_or_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        mode_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
    model.eval()
    return tokenizer, model


tokenizer, model = get_model()

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []


for msg in st.session_state.messages:
    st.chat_message("user").write(msg[0])
    st.chat_message("assistant").write(msg[1])


if prompt := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)
    prompt = '[' + prompt + ']'
    response, history = model.chat(
        tokenizer, prompt, meta_instruction=system_prompt, history=st.session_state.messages)
    st.session_state.messages.append((prompt, response))
    st.chat_message("assistant").write(response)