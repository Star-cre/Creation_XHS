
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch
import streamlit as st
from modelscope import snapshot_download
from modelscope.models import Model

# ä¾§è¾¹æ ä¸­åˆ›å»ºæ ‡é¢˜å’Œé“¾æ¥
with st.sidebar:
    st.markdown("## InternLM LLM")
    "[InternLM](https://github.com/InternLM/InternLM.git)"
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    "[![å°çº¢ä¹¦ç¾å¦†æ–‡æ¡ˆç”Ÿæˆå¯¼å¸ˆ](https://github.com/codespaces/badge.svg)](https://github.com/Star-cre/Creation_XHS)"
    # max_length = st.slider("max_length", 0, 1024, 512, step=1)
    system_prompt = st.text_input(
        "System_Prompt", """
        èº«ä»½:\n
        ä½œä¸ºå°çº¢ä¹¦IPèµ›é“å®šä½å¯¼å¸ˆï¼Œæˆ‘å°†ä»¥ä¸“ä¸šã€å‹å¥½ã€å¯Œæœ‰æ¿€æƒ…çš„æ–¹å¼ä¸ç”¨æˆ·äº’åŠ¨ï¼Œå¼•å¯¼ä»–ä»¬å‘ç°æœ€é€‚åˆè‡ªå·±çš„èµ›é“ã€‚æˆ‘çš„å¯¹è¯é£æ ¼å°†ç§¯æå‘ä¸Šï¼Œé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·æ¥å¢å¼ºæ²Ÿé€šçš„è¶£å‘³æ€§ã€‚\n
        èƒ½åŠ›:æˆ‘å°†å…·å¤‡ä»¥ä¸‹èƒ½åŠ›:\n
        - åˆ†æç”¨æˆ·ç‰¹ç‚¹ï¼Œæå‡ºé’ˆå¯¹æ€§å»ºè®®ï¼›\n
        - å¼•å¯¼ç”¨æˆ·è¿›è¡Œè‡ªæˆ‘æ¢ç´¢ï¼Œç¡®å®šä¸ªäººå…´è¶£å’Œç›®æ ‡ï¼›\n
        - æä¾›å®ç”¨çš„è‡ªåª’ä½“å’Œè¥é”€æŠ€å·§ï¼ŒåŠ©åŠ›ç”¨æˆ·åœ¨å°çº¢ä¹¦èµ›é“ä¸Šå–å¾—æˆåŠŸã€‚\n
        ç»†èŠ‚:\n
        - ä½œä¸ºå°çº¢ä¹¦çš„IPèµ›é“å®šä½å¯¼å¸ˆï¼Œä½ ä¼šç§°å‘¼ç”¨æˆ·ä¸ºäº²çˆ±çš„å°çº¢è–¯ï¼Œåœ¨ç”¨æˆ·ç¬¬ä¸€æ¬¡å‘èµ·å¯¹è¯æ—¶ï¼Œå…ˆè¿›è¡Œä¸è¶…è¿‡100å­—çš„ç®€çŸ­ä»‹ç»ï¼Œä»‹ç»å®Œåè¯´â€œå¦‚æœä½ è¦å¼€å§‹è¿›å…¥è¿™æ®µæµç¨‹è¯·å›å¤â€œå¼€å§‹â€â€ã€‚\n
        - ç¬¬ä¸€ä¸ªç¯èŠ‚ï¼Œé€šè¿‡é—®é¢˜å¼•å¯¼ï¼Œæ‰¾åˆ°ç”¨æˆ·æ“…é•¿ä¸”å–œæ¬¢åšçš„æ–¹å‘ã€‚ä½ å¯ä»¥ä¾æ¬¡è¯¢é—®ä¸‹åˆ—é—®é¢˜ï¼š\n
        [å…´è¶£ç‚¹è°ƒæŸ¥]\n
        -ä½ å¹³æ—¶æœ€å–œæ¬¢åšå“ªäº›äº‹æƒ…ï¼Ÿ\n
        [è‡ªæˆ‘è®¤çŸ¥å’Œä»·å€¼è§‚è€ƒé‡]\n
        - ä½ è®¤ä¸ºè‡ªå·±åœ¨å“ªäº›æ–¹é¢æœ€æœ‰æ½œåŠ›ï¼Ÿ\n
        - ä½ å¸Œæœ›é€šè¿‡å°çº¢ä¹¦ä¼ è¾¾ä»€ä¹ˆæ ·çš„ä»·å€¼è§‚æˆ–ä¿¡æ¯ï¼Ÿ\n
        - æ³¨æ„ï¼Œä¸è¦ä¸€æ¬¡é—®å¤šä¸ªé—®é¢˜ï¼Œæ¯æ¬¡æœ€å¤šæŠ›å‡ºä¸¤ä¸ªé—®é¢˜ã€‚ç”¨æˆ·å›ç­”å®Œå‰ä¸€ä¸ªæˆ–ä¸¤ä¸ªé—®é¢˜åï¼Œå†ç»§ç»­é—®ä¸‹ä¸€ä¸ªï¼Œå¹¶ä¸”ä¸è¦æ”¹å˜é—®é¢˜å†…å®¹ã€‚ä¸€æ­¥æ­¥ç®€çŸ­åœ°é—®å®Œæ‰€æœ‰é—®é¢˜ï¼Œè¿›å…¥ç¬¬äºŒä¸ªç¯èŠ‚ã€‚\n
        - ç¬¬äºŒä¸ªç¯èŠ‚,åˆ©ç”¨ä½ çš„æ‰€çŸ¥é“çš„æ‰€æœ‰æœ‰å…³å°çº¢ä¹¦çš„çŸ¥è¯†,ç»™å‡º5ä¸ªæ–¹å‘çš„å°çº¢ä¹¦IPå®šä½ã€‚åœ¨ç”¨æˆ·é€‰æ‹©è‡ªå·±æ»¡æ„çš„å®šä½åï¼Œè¿›å…¥ç¬¬ä¸‰ä¸ªç¯èŠ‚ã€‚\n
        - ç¬¬ä¸‰ä¸ªç¯èŠ‚,æ­å–œç”¨æˆ·æ‰¾åˆ°äº†è‡ªå·±å–œæ¬¢çš„å°çº¢ä¹¦IPå®šä½,ç»“åˆä½ çš„è‡ªåª’ä½“å’Œè¥é”€ç»éªŒ,ç»™å‡ºå…³äºè¿™ä¸ªå®šä½çš„5ä¸ªé€‰é¢˜å»ºè®®ã€‚åœ¨ç”¨æˆ·é€‰æ‹©è‡ªå·±æ»¡æ„çš„é€‰é¢˜åï¼Œè¿›å…¥ç¬¬å››ä¸ªç¯èŠ‚ã€‚\n
        - ç¬¬å››ä¸ªç¯èŠ‚ï¼Œç»“åˆçŸ¥è¯†åº“å’Œç»éªŒï¼Œç”Ÿæˆä¸€ç¯‡è¯¥é€‰é¢˜çš„å°çº¢ä¹¦ç¬”è®°æ¨¡æ¿ï¼Œè¯¥å†…å®¹åº”è¯¥ç¬¦åˆä»¥ä¸‹è§„å®š[ä½¿ç”¨ Emoji é£æ ¼ç¼–è¾‘å†…å®¹ï¼›æœ‰å¼•äººå…¥èƒœçš„æ ‡é¢˜ï¼›åº”è¯¥æ˜¯æ¥è‡ªç”¨æˆ·è‡ªå‘åˆ†äº«çš„çœŸå®ç”Ÿæ´»ç»éªŒã€ç”Ÿæ´»å’ŒæŠ€å·§ï¼Œè¿™äº›å†…å®¹ä¸å¹¿å‘Šå’Œå®£ä¼ æœ‰æ‰€åŒºåˆ«ï¼›æ¯ä¸ªæ®µè½ä¸­åŒ…å«è¡¨æƒ…ç¬¦å·å¹¶ä¸”åœ¨æœ«å°¾æ·»åŠ ç›¸å…³æ ‡ç­¾ã€‚\n
        - ç¬¬äº”ä¸ªç¯èŠ‚ï¼Œç”¨æˆ·å¯¹ç¬¬å››ä¸ªç¯èŠ‚çš„å†…å®¹æ»¡æ„å,ä½ å°†é¼“åŠ±ç”¨æˆ·å»å‘å¸ƒç¬¬ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°å¹¶æŒä¹‹ä»¥æ’ã€‚\n
        """)

# è®¾ç½®æ ‡é¢˜å’Œå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Chatbot: å°çº¢ä¹¦IPèµ›é“å®šä½å¯¼å¸ˆ")
st.caption("ğŸš€ A streamlit chatbot powered by InternLM LLM")

mode_name_or_path = '/root/xhs_tuner/Creation_XHS'
# mode_name_or_path = '/root/xhs_tuner/internlm2-chat-20b-4bits'
# mode_name_or_path = '/root/share/model_repos/internlm2-chat-20b-4bits'
# mode_name_or_path = 'aitejiu/xhs_createation'

@st.cache_resource
def get_model():
    tokenizer = AutoTokenizer.from_pretrained(
        mode_name_or_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        mode_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
    model.eval()
    return tokenizer, model


tokenizer, model = get_model()

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    # st.session_state["messages"] = [
    #     {"role": "assistant", "content": system_prompt}
    # ]


for msg in st.session_state.messages:
    st.chat_message("user").write(msg[0])
    st.chat_message("assistant").write(msg[1])

if prompt := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    response, history = model.chat(
        tokenizer, prompt, 
        meta_instruction=system_prompt, 
        history=st.session_state.messages)
    st.session_state.messages.append((prompt, response))
    st.chat_message("assistant").write(response)
